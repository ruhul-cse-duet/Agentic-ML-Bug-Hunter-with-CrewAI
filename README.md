# ğŸ§  Agentic ML Bug Hunter with CrewAI

An intelligent bug hunting system for Machine Learning projects powered by CrewAI and Local LLM (Ollama). This system uses multiple specialized AI agents to analyze, debug, and optimize your ML code.

## âœ¨ Features

- ğŸ” **Runtime Error Analysis**: Detect CUDA, tensor, and memory errors
- ğŸ§© **ML Logic Review**: Find silent bugs in model architecture
- ğŸ”§ **Automatic Code Fixes**: Generate production-ready patches
- âš¡ **Performance Optimization**: Improve training speed and memory usage
- ğŸ’» **Beautiful UI**: Modern, responsive web interface
- ğŸ¤– **Local LLM**: Runs entirely on your machine using Ollama
- ğŸ¯ **Multi-Agent System**: 4 specialized AI agents working together

## ğŸ—ï¸ Architecture

The system uses 4 specialized AI agents working together:

1. **Runtime Error Debugger**: Analyzes runtime errors and tracebacks
2. **ML Logic Reviewer**: Detects logical bugs in model design
3. **Code Fix Generator**: Creates clean code patches
4. **Performance Optimizer**: Suggests speed and memory optimizations

## ğŸ“‹ Prerequisites

- Python 3.11.14 or higher
- Ollama installed and running
- At least one Ollama model downloaded (llama2:7b recommended)
- 8GB RAM minimum (16GB recommended)
- Windows 10/11, Linux, or macOS

## ğŸš€ Installation

### Step 1: Clone/Navigate to Project
```bash
cd "E:\Data Science\ML_and_DL_project\NLP Project\Agentic Crewai Bug Hunter for ML Projects"
```

### Step 2: Run Setup (One-time)
```bash
# Double-click or run:
setup.bat
```

This will:
- âœ… Create virtual environment
- âœ… Install all Python dependencies
- âœ… Prepare the project

### Step 3: Install & Setup Ollama

1. **Download Ollama**: https://ollama.ai/download

2. **Install and start Ollama**:
```bash
# In a new terminal:
ollama serve
```

3. **Pull a model** (choose one):
```bash
# Recommended (3.8 GB) - Best quality
ollama pull llama2:7b

# OR Faster option (1.6 GB) - Good for testing
ollama pull gemma2:2b
```

4. **Verify installation**:
```bash
ollama list
```

## ğŸ® Usage

### Quick Start (Recommended)

Simply double-click:
```
run_local.bat
```

This will:
- âœ… Check all prerequisites
- âœ… Verify Ollama is running
- âœ… Check model availability
- âœ… Start the application
- âœ… Open at http://localhost:8000

### Manual Start

```bash
# Activate virtual environment
venv\Scripts\activate

# Ensure Ollama is running (in separate terminal)
ollama serve

# Start application
cd app
python main.py
```

### Access the Web Interface

Open your browser and navigate to:
```
http://localhost:8000
```

### Analyze your code

1. Paste your ML code or error logs into the text area
2. Click "Analyze with AI"
3. Wait 30-120 seconds for AI agents to process
4. Review the comprehensive debug report
5. Copy fixes and apply them to your code

## ğŸ“ Project Structure

```
Agentic Crewai Bug Hunter for ML Projects/
â”‚
â”œâ”€â”€ agents/                    # AI Agent definitions
â”‚   â”œâ”€â”€ runtime_agent.py      # Runtime error debugger
â”‚   â”œâ”€â”€ logic_agent.py         # ML logic reviewer
â”‚   â”œâ”€â”€ fix_agent.py           # Code fix generator
â”‚   â””â”€â”€ performance_agent.py   # Performance optimizer
â”‚
â”œâ”€â”€ app/                       # Application code
â”‚   â”œâ”€â”€ main.py               # FastAPI application
â”‚   â””â”€â”€ crew_runner.py        # CrewAI orchestration
â”‚
â”œâ”€â”€ prompts/                   # Agent system prompts
â”‚   â”œâ”€â”€ runtime.txt           # Runtime analysis prompt
â”‚   â”œâ”€â”€ logic.txt             # Logic review prompt
â”‚   â”œâ”€â”€ fix.txt               # Fix generation prompt
â”‚   â””â”€â”€ performance.txt        # Performance prompt
â”‚
â”œâ”€â”€ static/                    # Frontend assets
â”‚   â”œâ”€â”€ style.css             # Responsive CSS
â”‚   â””â”€â”€ javascript.js         # Interactive JS
â”‚
â”œâ”€â”€ templates/                 # HTML templates
â”‚   â””â”€â”€ index.html            # Main UI
â”‚
â”œâ”€â”€ config.py                 # Configuration management
â”œâ”€â”€ llm_model.py              # LLM initialization
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.bat                 # Setup script
â”œâ”€â”€ run_local.bat             # Local run script
â”œâ”€â”€ Dockerfile                # Docker configuration
â”œâ”€â”€ docker-compose.yml        # Docker Compose setup
â”‚
â””â”€â”€ Documentation/
    â”œâ”€â”€ README.md             # This file
    â”œâ”€â”€ QUICKSTART.md         # Quick start guide
    â”œâ”€â”€ TESTING.md            # Testing guide
    â””â”€â”€ COMPLETION.md         # Project summary
```

## ğŸ› ï¸ Configuration

### Change Model

Edit `.env` file:
```ini
# Use llama2 (recommended)
OLLAMA_MODEL=ollama/llama2:7b

# OR use gemma2 (faster)
OLLAMA_MODEL=ollama/gemma2:2b
```

### Adjust Model Parameters

```ini
TEMPERATURE=0.4        # 0.0 = deterministic, 1.0 = creative
MAX_TOKENS=512         # Maximum response length
```

### Customize Agent Behavior

Modify prompt files in the `prompts/` directory to customize agent behavior.

## ğŸ’¡ Example Use Cases

### 1. Debug PyTorch Runtime Errors
```python
import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(3, 64, 3)
        self.fc = nn.Linear(64, 10)  # Wrong dimension!
    
    def forward(self, x):
        x = self.conv(x)
        x = self.fc(x)  # Shape mismatch!
        return x
```

### 2. Optimize Training Performance
```python
# Slow training loop
for epoch in range(epochs):
    for data in dataloader:
        output = model(data)
        loss.backward()
        optimizer.step()
```

### 3. Fix Logic Errors
```python
# Wrong loss for classification
criterion = nn.MSELoss()  # Should be CrossEntropyLoss!
```

## ğŸ¯ Tips for Best Results

1. **Include Context**: Provide complete code snippets with imports
2. **Add Error Logs**: Paste full tracebacks for runtime errors
3. **Specify Framework**: Mention if using PyTorch, TensorFlow, etc.
4. **Model Details**: Include model architecture and training setup
5. **Be Specific**: Describe what's not working as expected

## ğŸ”§ Troubleshooting

### Ollama Connection Error
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama
ollama serve
```

### Model Not Found
```bash
# Pull the model again
ollama pull llama2:7b

# List available models
ollama list
```

### Port Already in Use
```bash
# Use a different port
# Edit app/main.py and change port=8000 to port=8001
uvicorn main:app --port 8001
```

### Slow Analysis
- Ensure Ollama is using GPU acceleration if available
- Try a smaller/faster model (gemma2:2b)
- Check system resources (RAM, GPU memory)
- Reduce MAX_TOKENS in .env file

## ğŸ“Š Performance

- **Analysis Time**: 30-120 seconds (depends on code complexity)
- **Memory Usage**: 2-4GB (model dependent)
- **GPU**: Recommended for faster inference
- **CPU**: Works but slower

## ğŸ³ Docker Support

### Build and run with Docker Compose

```bash
docker-compose up --build
```

### Or without Docker Compose:
```bash
docker build -t ml-bug-hunter .
docker run -p 8000:8000 ml-bug-hunter
```

**Note**: Docker setup requires Ollama to be running on host machine.

## ğŸ§ª Testing

See [TESTING.md](TESTING.md) for comprehensive testing guide.

Quick test:
```bash
# Verify system
verify.bat

# Run application
run_local.bat
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ™ Acknowledgments

- **CrewAI** for the multi-agent framework
- **Ollama** for local LLM inference
- **FastAPI** for the web framework
- **LiteLLM** for unified LLM interface

## ğŸ“§ Support

For issues, questions, or suggestions:
- Open an issue on GitHub
- Check [TESTING.md](TESTING.md) for troubleshooting
- Review [QUICKSTART.md](QUICKSTART.md) for quick help

## ğŸŒŸ Features

- âœ… **Complete Privacy**: Everything runs locally
- âœ… **No API Keys Required**: Uses local Ollama models
- âœ… **Fast Analysis**: Multi-agent parallel processing
- âœ… **Production Ready**: Clean, documented code
- âœ… **Easy Setup**: One-click installation and run
- âœ… **Docker Support**: Containerized deployment option

---

**Made by â¤ï¸[Ruhul Amin](https://www.linkedin.com/in/ruhul-duet-cse/)â¤ï¸ ML Engineers and Researchers**

**Happy Debugging! ğŸ§ âœ¨ğŸ”**
